# Config for FSDP

# Arch: SwiGLU No Bias
# Modalities: Mix of rgb2all, all2all, caption/T5-biased2all, and C4 text-only
# Datasets: Mix of COYO700M, CC12M, and C4
# To be run on 128 GPUs for batch size = 4096
run_name: auto

# Input & output
num_input_tokens: 256
num_target_tokens: 256
loss_type: mod

# Architecture
model: fm_xlarge_24e_24d_swiglu_nobias
patch_size: 16
input_size: 224
dtype: bfloat16
tokenizer_path: "fourm/utils/tokenizer/trained/text_tokenizer_4m_wordpiece_30k.json"

# Initialization
finetune: '/path/to/4M_checkpoint.pth' # Change me. Initialize 4M-21 training from 4M-7 checkpoint

# Train
epochs: -1
total_tokens: 500 # in billions
opt: adamw
blr: 0.00002 # this is base_lr = 2e-5, lr = base_lr * batch_size / 256
min_blr: 0.
warmup_epochs: -1
warmup_tokens: 10 # in billions
batch_size: 32 # 32 x 128 = 4096
clip_grad: 3.0 # With FSDP, set clip grad to a high value to obtain grad norm logs without impacting training dynamics.
use_act_checkpoint: True
skip_nan_grad: True

# Data
data_config: "cfgs/default/4m/data/cc12m+coyo+c4/main/mix_mod21_all2allmix_rgb2all_capT5bias_C4.yaml"
s3_data_endpoint: "/path/to/endpoint" # Change me
eval_freq: 1
fixed_eval: True
epoch_size: 10_000_000 # Number of samples per "epoch"

# Saving
save_ckpt_freq: 1 # Change if needed
output_dir: 'output/auto' # Change if needed

# Wandb
log_wandb: False # Set to True to log to Weights & Biases
wandb_project: '4m-train'
wandb_entity: null # Change if needed
wandb_run_name: auto
