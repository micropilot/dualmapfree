run_name: auto

# Architecture
encoder_type: vit_b_enc
input_size_min: 224
input_size_max: 224
input_size_enc: 224 # Only used for initializing frozen encoder posemb
resolution_step: 16
input_size_sd: 512
patch_size: 16 # encoder ViT patch size
max_input_size: True # Use max(input_size_max, input_size_sd) as target_size of dataloader

# Start from pre-trained checkpoint
full_ckpt: '/path/to/pretrained_weights' # Change me
freeze_enc: True

# Quantizer
codebook_size: 1024
latent_dim: 32
norm_codes: True
codebook_weight: 1.0
quantizer_type: lucid
coef_ema_dead_code: 32.0
code_replacement_policy: batch_random
commitment_weight: 1.0
quantizer_ema_decay: 0.99
kmeans_init: False

# Diffusion
cls_free_guidance_dropout: 0.0 # How often to drop out the conditioning
masked_cfg: False # Randomly mask out the tokens when using conditioning dropout
clip_sample: True # Only for logged images
loss_fn: mse
pretrained_cn: True
sd_path: runwayml/stable-diffusion-v1-5

# Train
dtype: bf16 # fp32, fp16 or bf16
epochs: 10
opt: adamw
opt_betas: [0.9, 0.99]
blr: 0.00016 # base_lr = 0.00016, lr = base_lr * batch_size / 256
warmup_lr: 0.000001 # 1e-6
min_lr: 0.
warmup_epochs: 1
batch_size: 16 # per GPU
hflip: 0.5
clip_grad: 1.0
model_ema: False
find_unused_params: True
save_ckpt_freq: 1
use_xformer: True

# Eval
step_eval: True
epoch_eval: False
eval_freq: 10000
eval_metrics_freq: 10000
eval_image_log_freq: 5000
num_eval_metrics_samples: 50000 # Number of samples to use for evaluating image metrics during training.
num_eval_timesteps: 50
input_size_eval: 224
guidance_scale: 2.5
cond_scale: 0.8

# Data
domain: rgb
imagenet_default_mean_and_std: False # Normalize to [-1,1]
min_crop_scale: 0.8
include_caption: False
data_path: '/path/to/dataset' # Change me
eval_data_path: '/path/to/eval_dataset' # Change me

# Wandb
log_wandb: False # Set to True to log to Weights & Biases
wandb_project: '4m-tokenizers'
wandb_entity: null # Change if needed
wandb_run_name: auto
output_dir: 'output/auto'