{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "current_folder = globals()['_dh'][0]\n",
    "os.chdir(os.path.dirname(os.path.abspath(current_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hitesh/challenge/dualmapfree/lib/models/MicKey/modules/ml_fourm'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(os.path.abspath(current_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install faiss via pip install faiss-gpu to perform retrieval.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.11 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n",
      "INFO:datasets:PyTorch version 2.0.1 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'detectron2'\n",
      "Detectron2 can be used for semseg visualizations. Please install detectron2 to use this feature, or plotting will fall back to matplotlib.\n",
      "No module named 'pyrender'\n",
      "Human pose dependencies are not installed, hence poses will not be visualized. To visualize them (optional), you can do the following: \n",
      "1) Install via `pip install timm yacs smplx pyrender pyopengl==3.1.4` \n",
      "   You may need to follow the pyrender install instructions: https://pyrender.readthedocs.io/en/latest/install/index.html \n",
      "2) Download SMPL data from https://smpl.is.tue.mpg.de/. See https://github.com/shubham-goel/4D-Humans/ for an example. \n",
      "3) Copy the required SMPL files (smpl_mean_params.npz, SMPL_to_J19.pkl, smpl/SMPL_NEUTRAL.pkl) to fourm/utils/hmr2_utils/data .\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from torchvision.utils import make_grid\n",
    "from tokenizers import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "except:\n",
    "    print('Please install faiss via pip install faiss-gpu to perform retrieval.')\n",
    "    \n",
    "from fourm.models.fm import FM\n",
    "from fourm.vq.vqvae import VQVAE, DiVAE\n",
    "from fourm.models.generate import GenerationSampler, build_chained_generation_schedules, init_empty_target_modality, init_full_input_modality, custom_text\n",
    "from fourm.data.modality_transforms import RGBTransform\n",
    "from fourm.data.modality_info import MODALITY_INFO\n",
    "from fourm.data.modality_transforms import MetadataTransform\n",
    "from fourm.utils.plotting_utils import decode_dict, visualize_bboxes, plot_text_in_square, decode_tok_depth, decode_tok_semseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7ff15c2655e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to False in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tok = Tokenizer.from_file('./fourm/utils/tokenizer/trained/text_tokenizer_4m_wordpiece_30k.json')\n",
    "\n",
    "toks = {\n",
    "    'tok_dinov2': VQVAE.from_pretrained('EPFL-VILAB/4M_tokenizers_DINOv2-B14_8k_224-448').eval().to(device),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = FM.from_pretrained('EPFL-VILAB/4M-21_XL').eval().to(device)\n",
    "sampler = GenerationSampler(fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_domains = ['rgb@224']\n",
    "target_domains = ['tok_dinov2@224'] \n",
    "tokens_per_target = [256]\n",
    "autoregression_schemes = ['roar']\n",
    "decoding_steps = [1]\n",
    "token_decoding_schedules = ['linear']\n",
    "temps = [0.01]\n",
    "temp_schedules = ['constant'] \n",
    "cfg_scales = [2.0]\n",
    "cfg_schedules = ['constant'] \n",
    "cfg_grow_conditioning = True\n",
    "top_p, top_k = 0.8, 0.0\n",
    "\n",
    "schedule = build_chained_generation_schedules(\n",
    "    cond_domains=cond_domains, target_domains=target_domains, tokens_per_target=tokens_per_target, autoregression_schemes=autoregression_schemes, \n",
    "    decoding_steps=decoding_steps, token_decoding_schedules=token_decoding_schedules, temps=temps, temp_schedules=temp_schedules,\n",
    "    cfg_scales=cfg_scales, cfg_schedules=cfg_schedules, cfg_grow_conditioning=cfg_grow_conditioning, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the transformations\n",
    "rgb_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def preprocess_image(image_tensor):\n",
    "    images = []\n",
    "    for img in image_tensor:\n",
    "        img_pil = transforms.ToPILImage()(img)\n",
    "        img_pil = rgb_transform(img_pil)\n",
    "        images.append(img_pil)\n",
    "    return torch.stack(images)\n",
    "\n",
    "img = torch.load(\"/home/hitesh/challenge/dualmapfree/te\")\n",
    "\n",
    "img_process = preprocess_image(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_sample = {\n",
    "    'rgb@224': {\n",
    "        'tensor': img_process.cuda(), # Batched tensor\n",
    "        'input_mask': torch.zeros(12, 196, dtype=torch.bool, device=device), # False = used as input, True = ignored\n",
    "        'target_mask': torch.ones(12, 196, dtype=torch.bool, device=device), # False = predicted as target, True = ignored\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize target modalities\n",
    "for target_mod, ntoks in zip(target_domains, tokens_per_target):\n",
    "    batched_sample = init_empty_target_modality(batched_sample, MODALITY_INFO, target_mod, 12, ntoks, device)\n",
    "    \n",
    "# Initialize input modalities\n",
    "for cond_mod in cond_domains:\n",
    "    batched_sample = init_full_input_modality(batched_sample, MODALITY_INFO, cond_mod, device, eos_id=text_tok.token_to_id(\"[EOS]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "out_dict = sampler.generate(\n",
    "    batched_sample, schedule, text_tokenizer=text_tok, \n",
    "    verbose=True, seed=0,\n",
    "    top_p=top_p, top_k=top_k,\n",
    ")\n",
    "dec_dict = decode_dict(\n",
    "    out_dict, toks, text_tok, \n",
    "    image_size=224, patch_size=16,\n",
    "    decoding_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 768, 16, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_dict['tok_dinov2@224'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mickey",
   "language": "python",
   "name": "mickey"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
